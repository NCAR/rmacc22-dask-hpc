{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5b442b-6da5-4084-b723-39f52c93eb6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Dask within an HPC Environment\n",
    "### RMACC 2022 Tutorial | 4 August, 2022\n",
    "\n",
    "<img src=images/NCAR-contemp-logo-blue.png width=500px alt=\"NCAR Logo\">\n",
    "\n",
    "Brian Vanderwende  \n",
    "HPC User Support Consultant  \n",
    "[vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9ef8e-60b0-493c-8462-be97e0c1c45e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Setting up Python to run Dask\n",
    "\n",
    "The `miniconda` package environment manager makes it very easy to deploy Dask anywhere.\n",
    "\n",
    "1. If conda is not available on your system, first install it:\n",
    "\n",
    "```\n",
    "# For example - on x86 Linux platforms\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "# Now start a new shell to finish conda integration into your environment\n",
    "```\n",
    "\n",
    "2. Use conda (or mamba) to create a new Python environment with required and [useful] packages:\n",
    "\n",
    "```\n",
    "conda create -n my-dask-env -c conda-forge dask dask-jobqueue [matplotlib python-graphviz jupyterlab ipywidgets dask-labextension globus-cli]\n",
    "```\n",
    "\n",
    "3. Activate the environment and run `python`, `ipython`, or use in Jupyter as a language kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e43fb-d8a2-475f-a59f-149c865cfd22",
   "metadata": {},
   "source": [
    "#### Downloading the dataset\n",
    "\n",
    "If you wish to follow along during or after the tutorial session, you will need to download the data we use in the following commands. I have made these data available via a Globus endpoint, which you can access at https://www.globus.org. Search for the *collection* called `RMACC 2022 - HPC Dask Tutorial` and download the data to the main directory of the GitHub repo (the one containing this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455ecd1-c333-498a-bad3-704bfc7c12b6",
   "metadata": {},
   "source": [
    "## Arrays, series, and dataframes - why we like Dask\n",
    "Complex data structures enable data science in Python:\n",
    "* [NumPy arrays](https://numpy.org/doc/stable/)\n",
    "* [Pandas series and dataframes](https://pandas.pydata.org/)\n",
    "* [XArray arrays](https://docs.xarray.dev/)\n",
    "\n",
    "*But datasets are getting larger all of the time! What if my data science is too big to fit into memory, or takes too long to complete an analysis?*\n",
    "\n",
    "### Dask increases the size of possible work from *fits-in-memory* to *fits-on-disk* (sometimes doing it faster) via distributed parallelism\n",
    "\n",
    "That said, **Dask should only be used when necessary as it incurs overhead.** Avoid Dask if you can easily:\n",
    "* Speed up your code with use of compiled routines in libraries like NumPy\n",
    "* Profile and optimize your serial code to minimize bottlenecks\n",
    "* Read in a subset of data to gain the insight you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024b3b5-a35d-41b7-99f3-b9d8485f8ce2",
   "metadata": {},
   "source": [
    "## Warming up with some Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73254248-9454-46f8-a84a-abedb87ce49f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make plots display inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f5b98-56f2-48ca-ab55-0d57ccaf39ea",
   "metadata": {},
   "source": [
    "At NCAR, we have created a nice utility called `qhist` to provide historical data on interactive and batch compute jobs.\n",
    "\n",
    "#### Load historical job data from 2022 June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b1213-8f8e-4da2-a5d7-7506a178ba2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "june_jobs = pd.read_csv('data/jobs-202206.dat')\n",
    "jj = june_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5abb3-7a7e-4069-aca4-aed6bb82a654",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the top five rows of the dataframe\n",
    "jj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4753a-5f98-4846-8172-6f3972de90c5",
   "metadata": {},
   "source": [
    "#### How many jobs did users run in June?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411d2c6-dee2-452b-a463-06578bd15d56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(jj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada36966-dcd8-43de-9498-a464fd55d99a",
   "metadata": {},
   "source": [
    "#### How many jobs ran in each queue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900aeb9f-c968-4497-b672-9c09f2267080",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a \"groupby object\" based on the selected columns\n",
    "# which can return info like the count of each type of the column value\n",
    "jj.groupby('Queue').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48475ac5-6ef2-4237-be5d-1dada8ee3230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we easily plot the prior data using matplotlib from pandas\n",
    "jj.groupby('Queue').size().plot.bar(logy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ae58d-c38d-425c-ac43-396d392b2c55",
   "metadata": {},
   "source": [
    "#### Where do Dask jobs run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7145d-123f-4412-a4e5-86c6f61c7198",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Select only one column of data\n",
    "jj['Job Name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768b89f-324a-4676-b674-76d7b807ebfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of each value in Job Name (similar to size above)\n",
    "jj['Job Name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd098d6-ac2c-41fa-8489-1bd9baa67126",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe of job entries with the Job Name \"dask-worker\"\n",
    "dj = jj[jj['Job Name'] == 'dask-worker']\n",
    "dj.groupby('Queue').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fac09-9bb0-4ccf-93c0-50930e269926",
   "metadata": {},
   "source": [
    "*Is this what we expect? Dask tasks tend to be **bursty, high-throughput** computing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5f54f-d6d1-48e0-9328-5f0d6139235b",
   "metadata": {},
   "source": [
    "#### What are typical worker CPU and memory needs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46f98-87a9-4d18-a93d-2028f112977a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# groupby operations can be on multiple columns and ones generated by manipulating a column\n",
    "dj.groupby(['NCPUs', dj['Req Mem (GB)'] // 10 * 10]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b2d11-3d33-40fc-908d-1fcb221809b3",
   "metadata": {},
   "source": [
    "#### How much of the request memory is wasted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a584b2-cb2e-4df1-bd9b-e09e47895ae3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new column\n",
    "dj['Unused Mem'] = dj['Req Mem (GB)'] - dj['Used Mem(GB)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5edc8e-5faf-45ed-80b3-3b05d105f06f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Check all datatypes of the dataframe (here we see usedmem did not get interpreted properly)\n",
    "dj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b6da8-67fb-4245-bb8b-6b7b82676e25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's cast used mem as the dtype that we want (float)\n",
    "dj['Used Mem(GB)'] = dj['Used Mem(GB)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d5b2e-6f6e-4ef7-9043-0d24d64c089c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# To avoid the above problem, we need to make sure we copy data in our dask job dataframe, and not reference it\n",
    "dj = jj[jj['Job Name'] == 'dask-worker'].copy()\n",
    "dj['Used Mem(GB)'] = dj['Used Mem(GB)'].astype(float)\n",
    "dj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a989d-2779-4dd9-b82a-2772f1b90986",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj['Unused Mem (%)'] = (dj['Req Mem (GB)'] - dj['Used Mem(GB)']) / dj['Req Mem (GB)'] * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ca032-22fa-41e7-84d8-1c6e3e1b2d67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj.groupby(dj['Unused Mem (%)'] // 10 * 10).size().plot.bar(xlabel = '% Unused', ylabel = 'Number of Jobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79eeeda-6806-4f75-9109-cdec5ee706bd",
   "metadata": {},
   "source": [
    "*Typically on busy systems, wasted resources means less job throughput, meaning...*  \n",
    "\n",
    "**LONGER WAIT TIMES :-(**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e1b41-1d08-4bf7-a3f6-10bd1c579803",
   "metadata": {},
   "source": [
    "#### How much of the requested walltime do workers actually use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffafe02-e7ba-4829-9465-ec78e5ea1638",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj['Job Start'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc606a8-546d-4475-aac5-235e4074fd35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This could also be done with a list comprehension!\n",
    "time_cols = ['Job Submit', 'Job Start', 'Job End']\n",
    "\n",
    "for col in time_cols:\n",
    "    dj[col] = pd.to_datetime(dj[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ae4b2-75c5-4024-9b62-4adf7f48be3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6da90-6715-489d-8a0e-2116d4491b79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj['elapsed'] = (dj['Job End'] - dj['Job Start']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58486e-d098-4717-9b43-98d2083059e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dj.groupby(dj['Walltime (h)'] // 1).mean()['elapsed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fd6ae-2592-4477-8ca2-3b64536fbb2b",
   "metadata": {},
   "source": [
    "#### How big is the June 2022 dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a2a40-2a5c-47aa-a08c-88c0bdd81d9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Shell commands can be run with \"!' in a notebook\n",
    "!ls -lh data/jobs-202206.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a3d9d-46cb-432e-8a32-1ddd1625be53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dataframe method to get metadata\n",
    "jj.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe9be5-403b-4689-a497-2256e21045ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Define function to display variable size in MiB\n",
    "def var_size(in_var):\n",
    "    result = sys.getsizeof(in_var) / 1024 / 1024\n",
    "    print(f\"Size of variable: {result:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63e638-96da-4346-9292-b2715b12869c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "var_size(jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b499128-3275-4324-a1e6-9d2d63b90e14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# deep option will include memory used by mutable objects\n",
    "jj.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3c056-6b82-47f8-a8cc-9195fb3004ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Scaling up to larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538c4ec-1d06-41ae-bbe3-ad788385e882",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!ls -lh data/jobs-*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01441e0e-ab5b-4003-b20a-b56d36b1bc38",
   "metadata": {},
   "source": [
    "#### Get the busiest day in the dataset\n",
    "Pandas can concatenate data to load data spread across multiple files:\n",
    "```\n",
    "import glob\n",
    "df = pd.concat(pd.read_csv(f) for f in glob.glob(\"data/jobs-*\"))\n",
    "```\n",
    "*However, this may exhaust memory if the data are large enough*  \n",
    "\n",
    "For this problem, we can first get the busiest day of each month, and then find the busiest among the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588f13d-5245-4fb4-9f21-d650ac4703a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import time, glob\n",
    "\n",
    "def busy_day(pbs_file):\n",
    "    print(pbs_file)\n",
    "    df = pd.read_csv(pbs_file, parse_dates = time_cols)\n",
    "    result = df.groupby(df['Job Start'].dt.date).size().agg(['idxmax','max'])\n",
    "    result = result.to_frame().transpose()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acfff1-c8eb-4c57-b35a-8636ef3d5bf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bd = pd.concat([busy_day(f) for f in glob.glob(\"data/jobs-*\")]).set_index('idxmax')\n",
    "bd['max'].astype(int).idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9c49c-0722-4a60-925d-11bddd3529ca",
   "metadata": {},
   "source": [
    "***Dask allows us to conceptualize all of these files as a single dataframe!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe91f45-fc61-463f-b312-9e1ed2b1d601",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's do a little cleanup (Python will not return the memory to the OS, however)\n",
    "del jj, dj, bd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17852bc2-0da7-4827-8ace-8db7792d9f24",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=images/dask_horizontal.svg width=800px alt=\"Dask Logo\"></img>  \n",
    "*Image credit: Anaconda, Inc. and contributors*\n",
    "\n",
    "## What is Dask?\n",
    "\n",
    "Dask is a Python package for scaling up data science workflows in parallel.\n",
    "\n",
    "* Features task-based parallelism\n",
    "* Can easily scale from 1 to 1000s of workers\n",
    "* Inherits subset of objects and methods from libraries like NumPy, Pandas, XArray, and scikit-learn\n",
    "* Prototype code serially and then easily parallelize using Dask\n",
    "* Smoothly run the same workflow on laptops to clusters via `schedulers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42943aa-e5bb-4124-9841-fe8130e2c78a",
   "metadata": {},
   "source": [
    "#### Dask collections\n",
    "\n",
    "A Dask *collection* is the fundamental thing we wish to parallelize. Most of the time, you will probably use one of the following *high-level* (big)data structures:\n",
    "\n",
    "| Collection | Serial | Dask |\n",
    "|-|-|-|\n",
    "| Arrays | numpy.array | dask.array.from_array |\n",
    "| Dataframes | pandas.read_csv | dask.dataframe.read_csv |\n",
    "| Unstructured | [1,2,3] | dask.bag.from_sequence([1,2,3]) |\n",
    "\n",
    "Dask also features two *low-level* collection types - `delayed` and `futures`.\n",
    "\n",
    "* **delayed** - run any arbitrary Python function using Dask task parallelism (think looped function calls)\n",
    "* **futures** - similar to delayed but allows for concurrency on the client (think backgrounded processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087002a-a2ec-44e7-bdd6-135417125714",
   "metadata": {},
   "source": [
    "#### Lazy evaluation\n",
    "\n",
    "Many programming languages (*including Python!*) use eager/strict evaluation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95357d22-17a7-41df-b808-2816d21d6588",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = 4\n",
    "x = x + 2\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce469e4d-a5ab-4ec8-9313-2a6933d0f2df",
   "metadata": {},
   "source": [
    "As soon as the cell is run, the assignment commands are executed and the value of `x` is changed.  \n",
    "\n",
    "Most Dask collections instead use *lazy evaluation*. When a command is run, a `delayed` object is returned that contains the *task graph* - providing instructions to the Dask workers on what to do. However, nothing happens until you tell it to with `.compute()`!\n",
    "\n",
    "*The one exception is Dask futures, which are executed as soon as the requisite data are available.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a251282-1695-4c7e-8e60-a6cdd6c35f38",
   "metadata": {},
   "source": [
    "#### Using Dask at its most basic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d79207-5392-45c0-8f2c-b9b1d1976d7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "narr = np.random.random((10000,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2904d-6b12-44ae-b813-cf81d7877ddc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the array into chunks for Dask to parallelize over\n",
    "darr = da.from_array(narr, chunks=(5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94071cab-7d3e-48a6-8c86-53494d174fd9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "var_size(narr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6ee2b-2f99-4aba-870b-791de07bf8c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Remember, this variable is only a facimile of the full array which will be split across workers\n",
    "var_size(darr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21358f-8a40-43dc-9f44-bb592f4d0896",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dask does give us ways to see the full size of the data (often much larger than your client machine!)\n",
    "print(\"Size of dataset:  {:.2f} MiB\".format(darr.nbytes / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d1901-9973-4fac-988c-8a708582c441",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232d5b9-484d-448d-8d8e-ec5fddd820db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The %%time magic measures the execution time of the whole cell\n",
    "narr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463f1ec-957d-43eb-81c2-5c81e2a4148b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remember, we are not doing any computation here, just constructing our task graph\n",
    "tg = darr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfa5cd-8790-4827-b391-8d8a393023bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tg.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3d170-2e11-4b9f-af52-40e49f838423",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "del narr, darr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7bd5b-535f-45e4-a8b8-161e9edf27c5",
   "metadata": {},
   "source": [
    "####\n",
    "![Dask flow](images/dask-overview.svg)  \n",
    "*Image credit: Anaconda, Inc. and contributors*\n",
    "---\n",
    "## Dask Distributed - using *clusters* to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e41f93-482a-4005-b729-662799f51bd6",
   "metadata": {},
   "source": [
    "#### The Dask scheduler - our task orchestrator\n",
    "\n",
    "When a computational task is submitted, the Dask distributed *scheduler* sends it off to a Dask *cluster* - simply a collection of Dask *workers*. A worker is typically a separate Python process on either the local host or a remote machine.  \n",
    "\n",
    "* **worker** - a Python interpretor which will perform work on a subset of our dataset\n",
    "* **cluster** - an object containing instructions for starting and talking to workers\n",
    "* **scheduler** - sends tasks from our task graph to workers\n",
    "* **client** - a local object that points to the scheduler (*often local but not always*)\n",
    "\n",
    "There are many ways to spread these components across local and remote computers, but we will focus on the most common in HPC contexts:\n",
    "\n",
    "1. Create a cluster pointing to local or scheduled remote resources\n",
    "2. Create a client pointing to a local scheduler\n",
    "3. Submit work to the client\n",
    "4. Run `.compute()` to begin worker execution\n",
    "5. Repeat steps 3+4 as necessary\n",
    "6. Shut down the Dask client and workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1dfb0f-54d3-4565-bc6a-ec79787785d8",
   "metadata": {},
   "source": [
    "## Let's return to our HPC job dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11531798-32c2-4e43-b301-ea674b24a148",
   "metadata": {},
   "source": [
    "#### Create a \"LocalCluster\" Client with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ace20-98ce-490b-b050-3d73acc54679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881b9b3-2fd5-4581-9b06-04b53b4dfb1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d99c02-42ca-4c66-b2a7-32207b8cda67",
   "metadata": {},
   "source": [
    "#### Load our jobs dataset into a Dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945ab50-bedb-41a3-8a55-7cf05f38e6c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "job_files = glob.glob('data/jobs-*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f2e03-9932-468c-9aad-6c9d7df26f2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Avoid reading the data in with pandas, even if it can fit within your client machine's memory\n",
    "djj = dd.read_csv(job_files, parse_dates = time_cols)\n",
    "djj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff289d3-36b4-4f7e-9ee3-7795c6eb26aa",
   "metadata": {},
   "source": [
    "* As promised, data has not yet been read into the dataframe (lazy evaluation)\n",
    "* So how does Dask know the `dtype` of each column?\n",
    "* Note the number of tasks... do they match the number of workers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72196988-b426-4edd-9b16-9e66c6f5c736",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "djj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a800f-abd8-462a-9f5a-dca7cf9d915e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We could use the Dask recommendation, but some inspection of the data reveals that\n",
    "# the value used for missing is the problem: the string '-'\n",
    "djj = dd.read_csv(job_files, parse_dates = time_cols, na_values = '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdc6f4-0173-459e-81c7-4eee306d1aa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "djj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a36434-d252-4512-9627-fac727e1c1b6",
   "metadata": {},
   "source": [
    "#### Now we can use Dask to find the busiest day in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950ded5-18e5-4860-9821-1d4de27b71de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Many of the pandas operations work the same way on a Dask dataframe\n",
    "top_day = djj.groupby(djj['Job Start'].dt.date).size()\n",
    "top_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26705418-dda3-4582-b333-89ac0c101892",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r = top_day.compute()\n",
    "r.agg(['idxmax','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9549ac0-08c9-4239-a419-7075913c36e7",
   "metadata": {},
   "source": [
    "#### What about the mean memory usage of Dask jobs by month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc9711-369d-4284-b20b-1f1bb73b9c6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "r = djj[djj['Job Name'] == 'dask-worker'].groupby(djj['Job Start'].dt.month).mean()['Used Mem(GB)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28f1b8-8762-48bf-9920-7d17f6a00e4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "r.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b5f7f-84f1-4d7c-82fd-89546b99d64d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Some operations allowed in pandas would not be reliable to run in parallel, so we must adjust our code\n",
    "ddj = djj[djj['Job Name'] == 'dask-worker']\n",
    "t = ddj.groupby([ddj['Job Start'].dt.year, ddj['Job Start'].dt.month]).mean()['Used Mem(GB)']\n",
    "r = t.compute().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c11fc-a78f-416f-a20b-95b72462e8ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b140a0-6037-4600-b640-4e2c07acc5ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This only uses the results data computed by Dask, so it should not launch any new parallel work\n",
    "r.plot.area(xlabel = 'Month', ylabel = 'Used Mem (GB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e783e-1c75-4d0a-a892-fe2828f518e7",
   "metadata": {},
   "source": [
    "#### Nice, but what did Dask do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1f406-9a55-4330-8791-572c69154b2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Requires ipywidgets\n",
    "t.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648676d-6170-482e-929b-e8ec94ba7cee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Requires python-graphviz (not pygraphviz!)\n",
    "t.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e2fa-2f17-49cc-a020-8a007c14ca30",
   "metadata": {},
   "source": [
    "Using a client with a `LocalCluster` gives us more insight and control over the simple thread pool approach\n",
    "\n",
    "* Set number of workers and threads-per-worker\n",
    "* *Plus many other configuration options!*\n",
    "* See the client / cluster diagnostics HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a717614-2a58-471f-82b0-12ec98954b64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Shutdown the client & local cluster\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90e9d3-f2c0-4dce-a07a-30cdbefc24ea",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Dask on an HPC cluster\n",
    "\n",
    "Fundamentally, the only thing we need to do to switch to a cluster environment is provide Dask with a way to access workers on other hosts.\n",
    "\n",
    "So we switch from a `LocalCluster` to a distributed one that uses SSH, Kubernetes, or a batch scheduler to access worker resources. Typical HPC environments will use  *Slurm*, *PBS*, *LSF*, etc.\n",
    "\n",
    "#### `dask-jobqueue` provides batch cluster types\n",
    "\n",
    "```\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(processes=1,\n",
    "                       threads=2,\n",
    "                       memory=\"10GB\",\n",
    "                       project=\"ABC1234\",\n",
    "                       walltime=\"01:00:00\",\n",
    "                       queue=\"htc\")\n",
    "```\n",
    "\n",
    "This provides a template with which the Dask and batch schedulers can spin up workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19043e-621c-4942-b326-0a379f2a0d7b",
   "metadata": {},
   "source": [
    "#### Let's start a PBS Cluster\n",
    "*The following cells are designed to run on the Casper system at NCAR. The steps from here on out will need to be modified for your particular system.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586a1d4-507a-451d-a66b-82c68668b794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b96ea-bde7-4dce-a485-fb6564d07c8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cluster = PBSCluster(\n",
    "     cores=1,\n",
    "     memory='2GiB',\n",
    "     processes=1,\n",
    "     local_directory='$TMPDIR/dask/spill',\n",
    "     resource_spec='select=1:ncpus=1:mem=2GB',\n",
    "     queue='casper',\n",
    "     walltime='10:00',\n",
    "     interface='ib0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420eaea5-3d1b-4087-9866-f3f813e66ed9",
   "metadata": {},
   "source": [
    "#### Validate the configuration by printing the job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d2274-8c6d-4ad1-9915-2bbe26b703cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa1562-a980-4d08-9d9c-b50fd13dc121",
   "metadata": {},
   "source": [
    "Things I'm looking for:\n",
    "1. PBS resource request matches what I'm assigning to the worker (e.g., memory units!)\n",
    "2. Where are my job logs going? (for debugging)\n",
    "3. Are there configuration settings I didn't expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef74ec5-a883-4547-9159-d3e39d6188df",
   "metadata": {},
   "source": [
    "#### Scaling the batch cluster\n",
    "\n",
    "By default, our `PBSCluster` starts with no active workers. We need to *scale* the cluster up to fit our needs. Clusters can be scaled number or resource. The following are equivalent:\n",
    "```\n",
    "cluster.scale(4)\n",
    "cluster.scale(cores=4)\n",
    "cluster.scale(memory='8 GB')\n",
    "```\n",
    "Running these commands will immediately submit batch jobs to start workers, so it's typical to wait until we're ready to do work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e96c06-90e9-4d85-9330-a7beb2553800",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#client = Client(cluster)\n",
    "# It can be nice to wait longer for client success on busy / sluggish systems\n",
    "client = Client(cluster, timeout=\"60s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55763a-6b90-4fa5-a958-78143efed519",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lets use our job scheduler to watch the workers spin up\n",
    "!qstat -u vanderwb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad45acd-de14-4781-a2d5-3f5eb49e152a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The wait blocks our notebook until the workers are ready - a synchronization point\n",
    "cluster.scale(4)\n",
    "client.wait_for_workers(4)\n",
    "client.ncores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e76da-45f4-461f-8eda-82a10817d071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!qstat -u vanderwb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38ce67-8a33-4310-8c52-f63c93723136",
   "metadata": {},
   "source": [
    "#### Now, let's again find the busiest day in our logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f2e43-c09b-445c-804d-e4deda66d110",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "job_files = glob.glob('data/jobs-*.dat')\n",
    "djj = dd.read_csv(job_files, parse_dates = time_cols, na_values = '-')\n",
    "top_day = djj.groupby(djj['Job Start'].dt.date).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7e6ac-528c-4ccc-ab80-f661cf7da92c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r = top_day.compute()\n",
    "r.agg(['idxmax','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ade95-8513-4d26-937e-f84c3fe0f8ab",
   "metadata": {},
   "source": [
    "#### What if we double the size of our cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a6dc0-f781-45f6-9bbf-11646f1a94a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cluster.scale(8)\n",
    "client.wait_for_workers(8)\n",
    "client.ncores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe47b4-2342-4ef6-a37a-5d16a8b6e5ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Are we creating 8 new workers here?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b61d65-2b12-40f8-b998-a555b497fd67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r = top_day.compute()\n",
    "r.agg(['idxmax','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d234a-77ab-4142-ae62-9fa0dcbc6019",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# It's always a good idea to scale down when done - otherwise we waste compute resources\n",
    "cluster.scale(0)\n",
    "client.ncores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59dbd6-2723-432e-b008-f0ba8511d00a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03df87-b34e-4067-9895-21fca4289fc1",
   "metadata": {},
   "source": [
    "#### Clusters can also adaptively scale\n",
    "\n",
    "For interactive, exploratory work, *adaptive scaling* can be useful. This allows the cluster to dynamically scale up and down based on the (Dask) scheduler's estimation of resource needs. This capability is highly customizable, but one basic method would be to set bounds on the number of worker jobs that can be used:\n",
    "```\n",
    "cluster.adapt(minimum=0, maximum=12)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9ab5f-6fed-4e9a-b9ba-8bb4aab97d61",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "* Ideally, you want to have more partitions than workers (*and evenly divisible!*) so that no workers remain idle for long\n",
    "* Using **processes** for workers avoids issues with Python's global interpretor lock (GIL) and is a reasonable default approach\n",
    "* Using **threads** for workers may be beneficial when running libraries that bypass the GIL with compiled code like NumPy  \n",
    "  *If you use threads, make sure you pay attention to settings like `OMP_NUM_THREADS` when using C extensions*\n",
    "* Very deep task graphs (millions of tasks) can have overhead in the hours - use larger partitions if possible in this scenario\n",
    "* When using a distributed cluster, manually **persist** reused data into worker memory (loading from RAM is much faster than disk)\n",
    "\n",
    "```\n",
    "# Here we persist the dataframe after setting the index, since that is a heavyweight operation we do not wish to repeat!\n",
    "df = dd.read_csv(data_files)\n",
    "df = df.set_index('timestep')\n",
    "df = client.persist(df)\n",
    "\n",
    "res1 = df.groupby('location').size().compute()\n",
    "res2 = df.groupby('location').mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd1930-bfd1-4386-803c-be1d64a28a6e",
   "metadata": {},
   "source": [
    "### Configuring Dask\n",
    "When scaling up from local (workstation) to distributed (HPC cluster) usage, it becomes very important to customize how Dask works. There are multiple ways to change configuration, from high to low precedence:\n",
    "\n",
    "1. Runtime setting within Python itself\n",
    "2. Environment variables before launching Python\n",
    "3. User-specific YAML config file in `~/.config/dask`\n",
    "4. python/etc/dask\n",
    "5. /etc/dask or $DASK_ROOT_CONFIG if set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100db4d-1e4f-4efe-a54c-832803021e24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# YAML configuration settings can be retrieved from within Python too\n",
    "from dask import config\n",
    "config.refresh()\n",
    "config.get('jobqueue.pbs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b597a2-b48b-4b22-ad53-5b82a227011d",
   "metadata": {},
   "source": [
    "## Logs and tracking the cluster and worker state\n",
    "\n",
    "A lot of information and debug output can be found in the worker logs. When using `dask-jobqueue`, these logs will be written to the job log location shown in the job script above. Write them to an easy to find location, and be mindful that you may generate a lot of files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e481b51-041f-4303-810b-b1e634e598c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!ls dask-worker-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bd30f-abdc-4fe1-bb17-436bf7a15a13",
   "metadata": {},
   "source": [
    "*Using a directory specific to the workflow, date, or job ID can help avoid log file confusion.*\n",
    "\n",
    "You can also print out the logs from **active** workers using the following client method:\n",
    "```\n",
    "client.get_worker_logs()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4852c4-1fe5-4b35-93b9-e9cc71f06fa4",
   "metadata": {},
   "source": [
    "#### Setting log verbosity\n",
    "Dask uses standard Python logging levels, shown here from least to most verbose:  \n",
    "```\n",
    "CRITICAL -> ERROR -> WARNING -> INFO -> DEBUG\n",
    "```\n",
    "These settings can be set in YAML config files or in the script. For example:\n",
    "```\n",
    "logging:\n",
    "  distributed.client: info\n",
    "  distributed.nanny: info\n",
    "  distributed.worker: debug\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0757659-9e61-4067-b6bc-73e19dde2dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some observations from NCAR support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e20f7-7094-40de-b7ee-3dd6647e16ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Workers take too long to start\n",
    "\n",
    "For any number of reasons, workers may take a while to start, causing the `nanny` to terminate the worker and your cluster scaling to fail. You may see messages like this in your logs:\n",
    "```\n",
    "2022-07-19 14:25:10,663 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.12.206.4:44646'.\n",
    "2022-07-19 14:25:10,667 - distributed.nanny - INFO - Nanny asking worker to close\n",
    "2022-07-19 14:25:13,617 - distributed.nanny - WARNING - Worker process still alive after 3.9999900817871095 seconds, killing\n",
    "2022-07-19 14:25:13,674 - distributed.dask_worker - INFO - Timed out starting worker\n",
    "2022-07-19 14:25:13,677 - distributed.dask_worker - INFO - End worker\n",
    "```\n",
    "**Solution:** increase the `death-timeout` parameter from 60 seconds to a higher number. Note that this parameter is used to terminate desync'd workers, so avoid setting it too high!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb261bc8-aaa0-4f3d-a1eb-2a2b7a1887e2",
   "metadata": {},
   "source": [
    "#### Worker is killed\n",
    "```\n",
    "KilledWorker: ('__call__-6af7aa29-2a09-45f3-a5e2-207c06562672', <Worker 'tcp://10.194.211.132:11927', memory: 0, processing: 1>) \n",
    "```\n",
    "\n",
    "There are many possibilities here unfortunately, but common ones can include out-of-memory conditions (easier when spilling to disk is disabled) or running out of disk space/quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca1811-5de3-463c-a4d7-a9aaf5e5ea38",
   "metadata": {},
   "source": [
    "#### Running your Dask tasks serially is powerful in debugging\n",
    "If you can temporarily switch to the non-distributed scheduler (e.g., you aren't using any asynchronous features like futures), you can run that one in single-thread mode. Then, you can use the Python debugger or the `%debug` cell magic in a notebook to examine the call stack upon an exception.\n",
    "```\n",
    "da.compute(scheduler=\"single-threaded\")\n",
    "```\n",
    "[This StackOverflow post](https://stackoverflow.com/questions/44193979/how-do-i-run-a-dask-distributed-cluster-in-a-single-thread) gives detail on running the distributed scheduler with a single thread, but it is not as simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b512a-c20a-421d-aa7d-6726ff8493a2",
   "metadata": {},
   "source": [
    "#### A not-insignificant number of errors are fixed by upgrading software..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77db2e9-e3be-4fe7-b1be-6e06883891f4",
   "metadata": {},
   "source": [
    "## Suggested Best Practices\n",
    "\n",
    "**General Dask use**\n",
    "* Before starting, analyze whether your problem truly justifies Dask overhead\n",
    "* If possible, prototype your code serially and then augment with distributed Dask\n",
    "\n",
    "**Dask configuration**\n",
    "* For reproducibility, ease of sharing, and clarity, put required settings in the script or notebook\n",
    "* Quality of life and/or community standard practices can go in user YAML config file\n",
    "\n",
    "**Larger scale considerations**\n",
    "* Many workers can put high load on network file systems in spill situations; disable spill or use local storage\n",
    "* Dask has *experimental* support for UCX for worker communication to use the full speed of a high-speed network\n",
    "\n",
    "**Resource utilization**\n",
    "* Choose generous memory defaults at first, **but then refine/reduce!**\n",
    "* Avoiding swapping to disk as much as possible; swap causes MAJOR performance penalties\n",
    "* Delete your cluster object once done with it to terminate idle worker jobs\n",
    "\n",
    "**Job turnaround**\n",
    "* Keep worker resource needs low: most times this will decrease scheduler wait times\n",
    "* Use adaptive scaling during exploratory work and manually scale in production\n",
    "* If your scheduler is tuned for large jobs, consider using [dask-mpi](http://mpi.dask.org/en/latest/) instead of dask-jobqueue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d05bb-e18b-40a4-8446-871af562256b",
   "metadata": {},
   "source": [
    "## Resources for further learning\n",
    "* [Dask documentation](https://docs.dask.org/en/stable/)\n",
    "* [Dask Distributed documentation](https://distributed.dask.org/en/stable/)\n",
    "* [Dask-jobqueue documentation](https://jobqueue.dask.org/en/latest/)\n",
    "* [Using Dask with GPUs](https://docs.dask.org/en/stable/gpu.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba201eb5-c6f4-4a6e-85b3-d259760cff3a",
   "metadata": {},
   "source": [
    "---\n",
    "## Addendum: Using JupyterLab on HPC systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e73de3-11af-4abd-8df3-194d73856fbe",
   "metadata": {},
   "source": [
    "Ideally, you have access to JupyterHub, which provides a web portal to notebooks, terminals, and Dask Distributed dashboards. If not, you will need to create SSH tunnels to forward the port for Jupyter *and, if desired, the Dask dashboard.*  \n",
    "\n",
    "**Remote System**\n",
    "```\n",
    "conda activate my-dask-env\n",
    "jupyter lab --no-browser [--port 8888]\n",
    "```\n",
    "**Local System**\n",
    "```\n",
    "ssh -N -L 8888:localhost:8888 remote.hpc.system.edu\n",
    "```\n",
    "Then, you would navigate to `http://localhost:8888` in your browser and sign into JupyterLab. Once you start a distributed dask cluster, you will then have its port number (8787 by default if unoccupied).  \n",
    "\n",
    "Fortunately, you do not need to forward the Dask cluster port, as Jupyter can proxy it for you. You instead can use the following URL in your browser:\n",
    "```\n",
    "http://localhost:<jupyter_port>/proxy/<cluster_port>/status\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-dask-env]",
   "language": "python",
   "name": "conda-env-my-dask-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
